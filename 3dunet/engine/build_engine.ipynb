{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import utils.calibrator as calibrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/home/jhpark/KISTI_Dockerimage/3dunet\"\n",
    "MODEL_NAME     = \"model\"\n",
    "MODEL_PATH     = os.path.join(ROOT, MODEL_NAME+'.onnx')\n",
    "MODEL_PB_PATH  = os.path.join(ROOT, 'model/config.pbtxt')\n",
    "\n",
    "# Model Input details\n",
    "MODEL_INPUT_NAME  = [\"PET\", \"CT\"]\n",
    "MODEL_INPUT_SHAPE = [(-1,1,128,128,160),(-1,1,128,128,160)]\n",
    "MODEL_OUTPUT_NAME = \"Segmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_DATATYPE        = trt.DataType.INT8\n",
    "TRT_CALIB_DATASET   = '/projects2/pi/jhpark/small_processed/test_samples'\n",
    "TRT_MAX_BATCH_SIZE  = 1\n",
    "TRT_ENGINE_PATH     = os.path.join(ROOT, f'engine/{MODEL_NAME}.engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT inference engine settings:\n",
      "  * Inference precision - DataType.INT8\n",
      "  * Max batch size - 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "# We first load all custom plugins shipped with TensorRT,\n",
    "# some of them will be needed during inference\n",
    "trt.init_libnvinfer_plugins(TRT_LOGGER, '')\n",
    "\n",
    "# TRT engine placeholder\n",
    "trt_engine = None\n",
    "\n",
    "# Display requested engine settings to stdout\n",
    "print(\"TensorRT inference engine settings:\")\n",
    "print(\"  * Inference precision - {}\".format(TRT_DATATYPE))\n",
    "print(\"  * Max batch size - {}\\n\".format(TRT_MAX_BATCH_SIZE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"precision\":\"int8\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = trt.IExecutionContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07/21/2022-17:11:49] [TRT] [W] onnx2trt_utils.cpp:369: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[07/21/2022-17:11:49] [TRT] [E] 4: [network.cpp::validate::2997] Error Code 4: Internal Error (PET: dynamic input is missing dimensions in profile 0.)\n",
      "[07/21/2022-17:11:49] [TRT] [E] 2: [builder.cpp::buildSerializedNetwork::636] Error Code 2: Internal Error (Assertion engine != nullptr failed. )\n"
     ]
    }
   ],
   "source": [
    "builder = trt.Builder(TRT_LOGGER)\n",
    "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "parser =trt.OnnxParser(network, TRT_LOGGER)\n",
    "\n",
    "config = builder.create_builder_config()\n",
    "profile = builder.create_optimization_profile()\n",
    "profile.set_shape_input(\"PET\", min=[-1,1,128,128,160], opt=[-1,1,128,128,160], max=[1,1,128,128,160]) \n",
    "profile.set_shape_input(\"CT\", min=[-1,1,128,128,160], opt=[-1,1,128,128,160], max=[1,1,128,128,160])\n",
    "config.add_optimization_profile(profile)\n",
    "\n",
    "with open(MODEL_PATH, \"rb\") as f:\n",
    "    parser.parse(f.read())\n",
    "\n",
    "inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
    "outputs = [network.get_output(i) for i in range(network.num_outputs)]    \n",
    "\n",
    "if cfg[\"precision\"] == \"fp16\":\n",
    "    \n",
    "    if builder.platform_has_fast_fp16:\n",
    "        print(\"FP16 is not supported natively\")\n",
    "    else:\n",
    "        config.set_flag(trt.BuilderFlag.FP16)\n",
    "elif cfg[\"precision\"] == \"int8\":\n",
    "\n",
    "    if not builder.platform_has_fast_int8:\n",
    "        print(\"INT is not supported natively\")\n",
    "\n",
    "    else:\n",
    "        config.set_flag(trt.BuilderFlag.INT8)\n",
    "        config.int8_calibrator = calibrator.SSDEntropyCalibrator(data_dir=TRT_CALIB_DATASET, cache_file=TRT_ENGINE_PATH+'INT8CacheFile')\n",
    "\n",
    "engine = builder.build_serialized_network(network, config)\n",
    "\n",
    "#with open(TRT_ENGINE_PATH, \"wb\") as f:\n",
    "#    print(\"Serializing engine to file: {:}\".format(TRT_ENGINE_PATH))\n",
    "#    f.write(engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('env_trex': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e76ea805bcc3e49c8effa690c20891b3be6fc3d2d6407a43bbe317590b63bbe4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
